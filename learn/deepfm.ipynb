{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c1ede8",
   "metadata": {},
   "source": [
    "DeepFM（Deep Factorization Machines）是一种用于CTR（点击通过率）预测的深度学习模型，它结合了传统的因子分解机（Factorization Machines, FM）模型和深度神经网络（Deep Neural Networks, DNN）。DeepFM 由华为诺亚方舟实验室提出，旨在同时学习低阶和高阶的特征交互，从而提高预测的准确性。这使得它在推荐系统、广告点击率预测等领域非常有效。\n",
    "\n",
    "主要特点和组成部分：\n",
    "\n",
    "1. **因子分解机组件（FM Component）**：负责学习特征的低阶交互。因子分解机是一种广泛用于推荐系统的算法，能够有效地处理稀疏数据并学习特征之间的交互。\n",
    "\n",
    "2. **深度神经网络组件（Deep Component）**：负责学习高阶的特征交互。这部分由多个隐藏层组成，可以捕捉复杂的非线性特征组合。\n",
    "\n",
    "3. **无缝集成**：DeepFM 的关键在于它将 FM 和 DNN 无缝集成在一个模型中，同时学习低阶和高阶特征交互，而不是像以前的模型那样将它们分开处理。\n",
    "\n",
    "4. **端到端训练**：整个模型可以通过端到端的方式进行训练，不需要预训练单独的组件。\n",
    "\n",
    "5. **应用广泛**：由于它结合了FM和深度学习的优势，DeepFM在推荐系统、在线广告、搜索排名等多个领域显示出了优异的性能。\n",
    "\n",
    "DeepFM 的成功在于它克服了一些传统模型的限制，例如仅依赖于人工特征工程或不能有效处理高阶特征交互的问题。通过结合深度学习，它能自动学习复杂的特征表示，提高了模型的泛化能力和准确性。\n",
    "\n",
    "在 TensorFlow 中实现 DeepFM 模型需要几个关键步骤。首先，您需要构建模型的两个主要部分：因子分解机（FM）部分和深度神经网络（DNN）部分。然后，您需要将这两部分结合起来，实现模型的整体架构。最后，您将编写训练循环，用于训练和评估模型。\n",
    "\n",
    "下面是一个简化的 DeepFM 模型的示例实现：\n",
    "\n",
    "1. **导入所需的库**：\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "```\n",
    "\n",
    "2. **定义 FM 和 DNN 组件**：\n",
    "\n",
    "```python\n",
    "def build_deepfm(feature_dim, field_dim, embed_dim, hidden_units, dropout_rate):\n",
    "    # 输入层\n",
    "    inputs = Input(shape=(field_dim,))\n",
    "\n",
    "    # FM 组件\n",
    "    # 线性部分\n",
    "    linear_part = Dense(1)(inputs)\n",
    "\n",
    "    # 嵌入层\n",
    "    embeddings = Embedding(feature_dim, embed_dim)(inputs)\n",
    "    sum_square = tf.square(tf.reduce_sum(embeddings, axis=1))\n",
    "    square_sum = tf.reduce_sum(tf.square(embeddings), axis=1)\n",
    "    fm_part = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)\n",
    "\n",
    "    # DNN 组件\n",
    "    deep_part = Flatten()(embeddings)\n",
    "    for units in hidden_units:\n",
    "        deep_part = Dense(units, activation='relu')(deep_part)\n",
    "        deep_part = Dropout(dropout_rate)(deep_part)\n",
    "    deep_part = Dense(1)(deep_part)\n",
    "\n",
    "    # 将 FM 和 DNN 组件相加\n",
    "    output = tf.keras.activations.sigmoid(linear_part + fm_part + deep_part)\n",
    "\n",
    "    # 构建模型\n",
    "    model = Model(inputs, output)\n",
    "    return model\n",
    "```\n",
    "\n",
    "3. **构建模型并编译**：\n",
    "\n",
    "```python\n",
    "# 模型参数\n",
    "feature_dim = 1000  # 特征维度\n",
    "field_dim = 10      # 字段维度\n",
    "embed_dim = 8       # 嵌入维度\n",
    "hidden_units = [128, 64]  # DNN隐藏层单元数\n",
    "dropout_rate = 0.5  # Dropout比率\n",
    "\n",
    "# 构建模型\n",
    "deepfm = build_deepfm(feature_dim, field_dim, embed_dim, hidden_units, dropout_rate)\n",
    "\n",
    "# 编译模型\n",
    "deepfm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "4. **训练模型**：\n",
    "\n",
    "```python\n",
    "# 假设 X_train 和 y_train 是您的训练数据和标签\n",
    "# deepfm.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.1)\n",
    "```\n",
    "\n",
    "以上代码是一个基础的 DeepFM 实现。在实际应用中，您可能需要根据具体的数据集和任务进行调整，例如特征处理、模型参数调整等。此外，模型的性能还可以通过超参数调优和更复杂的网络结构来进一步提升。\n",
    "\n",
    "\n",
    "我已经构造了一个简单的模拟数据集，包含1000个样本和10个特征。每个特征的值是一个0到999之间的整数。目标值（`target`）是根据这些特征的线性组合加上一些随机噪声生成的，并且被转换成了二元分类问题（0或1）。\n",
    "\n",
    "这是数据集的前几行的样子：\n",
    "\n",
    "| feature_0 | feature_1 | ... | feature_9 | target |\n",
    "|-----------|-----------|-----|-----------|--------|\n",
    "| 684       | 559       | ... | 723       | 0      |\n",
    "| 277       | 754       | ... | 705       | 0      |\n",
    "| 486       | 551       | ... | 72        | 1      |\n",
    "| ...       | ...       | ... | ...       | ...    |\n",
    "\n",
    "我们可以使用这个数据集来训练 DeepFM 模型。由于数据是随机生成的，模型的性能可能不会特别高，但它可以用作一个示例来展示模型的训练过程。您想要继续训练模型吗？\n",
    "\n",
    "\n",
    "要训练 DeepFM 模型，您首先需要将数据集划分为训练集和验证集。然后，使用 `deepfm.fit` 方法进行训练。这里是具体步骤：\n",
    "\n",
    "1. **划分数据集**：通常我们会将数据集分为训练集和验证集。训练集用于训练模型，而验证集用于评估模型的性能并调整超参数。\n",
    "\n",
    "2. **训练模型**：使用 `deepfm.fit` 方法进行训练。您可以指定批大小（`batch_size`）、训练轮次（`epochs`）以及验证集的比例（`validation_split`）。\n",
    "\n",
    "下面是如何实现这些步骤的代码：\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 划分训练集和验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.1, random_state=42)\n",
    "\n",
    "# 转换为 TensorFlow 格式\n",
    "X_train = X_train.values\n",
    "X_val = X_val.values\n",
    "y_train = y_train.values\n",
    "y_val = y_val.values\n",
    "\n",
    "# 训练模型\n",
    "deepfm.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))\n",
    "```\n",
    "\n",
    "请注意，上面的代码假设您已经定义了 `deepfm` 模型，并且您的环境已经安装了所有必要的库（如 TensorFlow、scikit-learn）。此外，由于数据是随机生成的，模型的性能可能与实际场景中的性能有所不同。这个示例的主要目的是展示如何实现和训练 DeepFM 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11cd5784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47ce93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deepfm(feature_dim, field_dim, embed_dim, hidden_units, dropout_rate):\n",
    "    # 输入层\n",
    "    inputs = Input(shape=(field_dim,))\n",
    "\n",
    "    # FM 组件\n",
    "    # 线性部分\n",
    "    linear_part = Dense(1)(inputs)\n",
    "\n",
    "    # 嵌入层\n",
    "    embeddings = Embedding(feature_dim, embed_dim)(inputs)\n",
    "    sum_square = tf.square(tf.reduce_sum(embeddings, axis=1))\n",
    "    square_sum = tf.reduce_sum(tf.square(embeddings), axis=1)\n",
    "    fm_part = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)\n",
    "\n",
    "    # DNN 组件\n",
    "    deep_part = Flatten()(embeddings)\n",
    "    for units in hidden_units:\n",
    "        deep_part = Dense(units, activation='relu')(deep_part)\n",
    "        deep_part = Dropout(dropout_rate)(deep_part)\n",
    "    deep_part = Dense(1)(deep_part)\n",
    "\n",
    "    # 将 FM 和 DNN 组件相加\n",
    "    output = tf.keras.activations.sigmoid(linear_part + fm_part + deep_part)\n",
    "\n",
    "    # 构建模型\n",
    "    model = Model(inputs, output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c9ae57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型参数\n",
    "feature_dim = 1000  # 特征维度\n",
    "field_dim = 10      # 字段维度\n",
    "embed_dim = 8       # 嵌入维度\n",
    "hidden_units = [128, 64]  # DNN隐藏层单元数\n",
    "dropout_rate = 0.5  # Dropout比率\n",
    "\n",
    "# 构建模型\n",
    "deepfm = build_deepfm(feature_dim, field_dim, embed_dim, hidden_units, dropout_rate)\n",
    "\n",
    "# 编译模型\n",
    "deepfm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c951af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>47</td>\n",
       "      <td>64</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>9</td>\n",
       "      <td>83</td>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>12</td>\n",
       "      <td>58</td>\n",
       "      <td>65</td>\n",
       "      <td>39</td>\n",
       "      <td>87</td>\n",
       "      <td>46</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81</td>\n",
       "      <td>37</td>\n",
       "      <td>25</td>\n",
       "      <td>77</td>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>80</td>\n",
       "      <td>69</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>64</td>\n",
       "      <td>82</td>\n",
       "      <td>99</td>\n",
       "      <td>88</td>\n",
       "      <td>49</td>\n",
       "      <td>29</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>32</td>\n",
       "      <td>65</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>32</td>\n",
       "      <td>31</td>\n",
       "      <td>74</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0         44         47         64         67         67          9   \n",
       "1         70         88         88         12         58         65   \n",
       "2         81         37         25         77         72          9   \n",
       "3         47         64         82         99         88         49   \n",
       "4         39         32         65          9         57         32   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  target  \n",
       "0         83         21         36         87       0  \n",
       "1         39         87         46         88       0  \n",
       "2         20         80         69         79       0  \n",
       "3         29         19         19         14       1  \n",
       "4         31         74         23         35       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 构建一个简单的模拟数据集\n",
    "np.random.seed(0)\n",
    "\n",
    "# 假设有1000个样本，每个样本10个特征，特征值为0-999的整数\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "\n",
    "# 生成特征数据\n",
    "X = np.random.randint(0, 100, size=(n_samples, n_features))\n",
    "\n",
    "# 生成目标值，简单使用线性组合的方式，再加上一些噪声\n",
    "true_weights = np.random.randn(n_features)\n",
    "y = np.dot(X, true_weights) + np.random.randn(n_samples) * 5\n",
    "\n",
    "# 将目标值转换为二元分类（0或1）\n",
    "y = (y > np.median(y)).astype(int)\n",
    "\n",
    "# 创建 DataFrame\n",
    "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)])\n",
    "df['target'] = y\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c395a5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 28.8838 - accuracy: 0.4956 - val_loss: 14.8977 - val_accuracy: 0.5900\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 12.9497 - accuracy: 0.5256 - val_loss: 13.8799 - val_accuracy: 0.5900\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 11.6325 - accuracy: 0.5389 - val_loss: 13.9034 - val_accuracy: 0.6000\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 10.3610 - accuracy: 0.5567 - val_loss: 13.6496 - val_accuracy: 0.6000\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 9.2354 - accuracy: 0.5844 - val_loss: 13.1611 - val_accuracy: 0.6100\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 8.5117 - accuracy: 0.5967 - val_loss: 13.1938 - val_accuracy: 0.6200\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 8.0398 - accuracy: 0.6222 - val_loss: 12.7183 - val_accuracy: 0.6400\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 7.4914 - accuracy: 0.6256 - val_loss: 12.4179 - val_accuracy: 0.6400\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 6.9853 - accuracy: 0.6489 - val_loss: 12.1505 - val_accuracy: 0.6400\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 6.6819 - accuracy: 0.6444 - val_loss: 11.9777 - val_accuracy: 0.6500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2cc9dd351d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 划分训练集和验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.1, random_state=42)\n",
    "\n",
    "# 转换为 TensorFlow 格式\n",
    "X_train = X_train.values\n",
    "X_val = X_val.values\n",
    "y_train = y_train.values\n",
    "y_val = y_val.values\n",
    "\n",
    "# 训练模型\n",
    "deepfm.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759d993",
   "metadata": {},
   "source": [
    "要使用 DeepFM 模型进行预测，您可以简单地使用模型的 `predict` 方法。通常，您会在一个新的数据集上进行预测，这个数据集应该有与训练数据相同的特征结构。下面是如何进行预测的步骤：\n",
    "\n",
    "1. **准备数据**：确保您的预测数据与训练数据在格式和结构上是一致的。如果有任何预处理步骤（例如标准化、编码等），也应该应用于预测数据。\n",
    "\n",
    "2. **进行预测**：使用 `predict` 方法对新数据进行预测。\n",
    "\n",
    "假设您有一些新数据（我们可以从原始数据集中随机抽取一些样本来模拟），预测步骤如下：\n",
    "\n",
    "```python\n",
    "# 假设 new_data 是需要预测的新数据\n",
    "# new_data = ...\n",
    "\n",
    "# 确保新数据的格式与训练数据一致\n",
    "# 如果需要，应用与训练数据相同的预处理步骤\n",
    "\n",
    "# 使用模型进行预测\n",
    "predictions = deepfm.predict(new_data)\n",
    "\n",
    "# 根据需要处理预测结果\n",
    "# 例如，您可以将预测结果转换为标签\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "```\n",
    "\n",
    "请注意，这里的 `new_data` 应该是一个 NumPy 数组或类似的数据结构，其形状和训练数据一致。此外，DeepFM 模型的输出是在 [0, 1] 范围内的概率值，因此您可能需要设定一个阈值（如 0.5）来确定最终的分类标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c8c45e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38, 65, 33, 97, 33,  3, 86, 50, 33,  3],\n",
       "       [39, 35, 28, 81, 37,  1, 81, 30, 34,  3],\n",
       "       [44,  2, 94, 69, 47, 76, 17, 79, 56, 41],\n",
       "       [80, 66, 72, 10, 90, 27, 56, 65, 64, 69],\n",
       "       [33, 43, 89,  1, 98, 56, 49, 40, 82, 41],\n",
       "       [52, 39, 83, 63,  7, 31, 96, 68, 27, 24],\n",
       "       [36, 38, 92, 85, 87, 37, 20, 13, 40, 50],\n",
       "       [97, 73,  6, 88, 23, 40,  5, 14, 87, 16],\n",
       "       [13, 86, 46, 99, 87, 37, 93, 90, 82, 86],\n",
       "       [ 9, 50, 97, 48, 12, 70, 36, 99, 36, 30]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_predict_samples = 10  # 预测样本数量\n",
    "predict_data = df.sample(n_predict_samples).drop('target', axis=1).values\n",
    "\n",
    "predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5503e507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 168ms/step\n"
     ]
    }
   ],
   "source": [
    "# 使用模型进行预测\n",
    "predictions = deepfm.predict(predict_data)\n",
    "\n",
    "# 根据需要处理预测结果\n",
    "# 例如，您可以将预测结果转换为标签\n",
    "predicted_labels = (predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d39c994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9fce15",
   "metadata": {},
   "source": [
    "DeepFM 模型结合了因子分解机（Factorization Machines, FM）和深度神经网络（Deep Neural Networks, DNN）的特点。模型的输出是 FM 部分和 DNN 部分的组合。具体来说，模型的计算公式可以分为以下几部分：\n",
    "\n",
    "1. **线性部分**：这是 FM 部分的线性回归部分，用于捕捉一阶特征交互。对于特征 \\( x \\) 和对应的权重 \\( w \\)，线性部分的计算公式为：\n",
    "$$ \\text{Linear} = w_0 + \\sum_{i=1}^{n} w_i x_i $$\n",
    "其中，$w_0$ 是偏置项，$n$ 是特征的数量。\n",
    "\n",
    "2. **因子分解机（FM）部分**：用于捕捉二阶特征交互。其计算公式为：\n",
    "\\[ \\text{FM} = \\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\langle v_i, v_j \\rangle x_i x_j \\]\n",
    "其中，\\( \\langle \\cdot, \\cdot \\rangle \\) 表示向量的点积，\\( v_i \\) 和 \\( v_j \\) 是特征 \\( i \\) 和 \\( j \\) 的嵌入向量。\n",
    "\n",
    "3. **深度神经网络（DNN）部分**：用于捕捉高阶特征交互。这部分由多个隐藏层组成，每层都应用激活函数，如 ReLU。对于第 \\( l \\) 层，其计算公式为：\n",
    "\\[ \\text{DNN}^{(l)} = \\text{ReLU}(W^{(l)} \\cdot \\text{DNN}^{(l-1)} + b^{(l)}) \\]\n",
    "其中，\\( W^{(l)} \\) 和 \\( b^{(l)} \\) 分别是第 \\( l \\) 层的权重和偏置，ReLU 是激活函数。\n",
    "\n",
    "4. **模型输出**：最后，将线性部分、FM 部分和 DNN 部分的输出相加，并通过一个 Sigmoid 函数得到最终的预测概率：\n",
    "\\[ \\text{Output} = \\sigma(\\text{Linear} + \\text{FM} + \\text{DNN}) \\]\n",
    "其中，\\( \\sigma \\) 是 Sigmoid 函数，用于将输出转换为概率值。\n",
    "\n",
    "在实际的 TensorFlow 实现中，这些部分会通过相应的层和操作来构建。例如，线性部分可以使用 `Dense` 层实现，FM 部分需要自定义嵌入和交叉项的计算，而 DNN 部分则是通过堆叠多个 `Dense` 层构建的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7504ba52",
   "metadata": {},
   "source": [
    "DeepFM模型结合了因子分解机（FM）的优势和深度神经网络（DNN）的优势。在FM部分，它考虑了特征的一阶（线性）和二阶（交互）关系。在DNN部分，它学习特征的高阶交互。下面是DeepFM模型各部分的详细说明及其对应的代码实现：\n",
    "\n",
    "### 1. 一阶（线性）部分\n",
    "一阶部分主要处理特征的线性组合。对于每个特征 \\( x_i \\)，模型学习一个权重 \\( w_i \\)。一阶部分的输出是所有特征及其对应权重的线性组合。\n",
    "\n",
    "一阶部分的计算公式是：\n",
    "\\[ \\text{Linear} = w_0 + \\sum_{i=1}^{n} w_i x_i \\]\n",
    "其中，\\( w_0 \\) 是偏置项，\\( w_i \\) 是特征 \\( x_i \\) 的权重。\n",
    "\n",
    "### 2. 二阶交互（FM）部分\n",
    "二阶交互部分考虑了特征间的交互作用。每个特征 \\( x_i \\) 被映射到一个低维空间（称为嵌入向量），模型学习特征对间的交互。\n",
    "\n",
    "二阶交互部分的计算公式是：\n",
    "\\[ \\text{FM} = \\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\langle v_i, v_j \\rangle x_i x_j \\]\n",
    "其中，\\( \\langle v_i, v_j \\rangle \\) 表示嵌入向量 \\( v_i \\) 和 \\( v_j \\) 的点积。\n",
    "\n",
    "### 3. 深度神经网络（DNN）部分\n",
    "DNN部分负责学习高阶特征交互。它由多个全连接层组成，每层都可以有不同数量的神经元和激活函数。\n",
    "\n",
    "DNN部分的计算公式（对于每一层）是：\n",
    "\\[ \\text{DNN}^{(l)} = \\text{Activation}(W^{(l)} \\cdot \\text{DNN}^{(l-1)} + b^{(l)}) \\]\n",
    "其中，\\( W^{(l)} \\) 和 \\( b^{(l)} \\) 分别是第 \\( l \\) 层的权重和偏置，Activation 是激活函数（如ReLU）。\n",
    "\n",
    "### 4. 模型输出\n",
    "最终，将一阶、二阶和DNN部分的输出相加，并通过一个激活函数（如Sigmoid）得到预测结果。\n",
    "\n",
    "模型输出的计算公式是：\n",
    "\\[ \\text{Output} = \\sigma(\\text{Linear} + \\text{FM} + \\text{DNN}) \\]\n",
    "其中，\\( \\sigma \\) 是Sigmoid函数，用于将输出转换为概率值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0c4108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_deepfm(feature_dim, field_dim, embed_dim, hidden_units, dropout_rate):\n",
    "    # 输入层\n",
    "    # field_dim 是输入特征的数量\n",
    "    inputs = Input(shape=(field_dim,))\n",
    "\n",
    "    # FM 组件的线性部分\n",
    "    # 这部分处理特征的一阶线性关系\n",
    "    linear_part = Dense(1)(inputs)\n",
    "\n",
    "    # FM 组件的二阶交互部分\n",
    "    # 首先，使用嵌入层将每个特征转换为一个低维向量\n",
    "    embeddings = Embedding(feature_dim, embed_dim)(inputs)\n",
    "\n",
    "    # 计算嵌入向量的和的平方\n",
    "    sum_square = tf.square(tf.reduce_sum(embeddings, axis=1))\n",
    "\n",
    "    # 计算嵌入向量的平方的和\n",
    "    square_sum = tf.reduce_sum(tf.square(embeddings), axis=1)\n",
    "\n",
    "    # 根据FM的公式计算二阶交互项\n",
    "    fm_part = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)\n",
    "\n",
    "    # DNN 组件\n",
    "    # 首先，将嵌入向量展平\n",
    "    deep_part = Flatten()(embeddings)\n",
    "\n",
    "    # 通过多个全连接层（Dense）处理特征\n",
    "    # hidden_units 包含每个隐藏层的神经元数量\n",
    "    for units in hidden_units:\n",
    "        deep_part = Dense(units, activation='relu')(deep_part)\n",
    "        # 在每个全连接层后添加Dropout，以防止过拟合\n",
    "        deep_part = Dropout(dropout_rate)(deep_part)\n",
    "\n",
    "    # 最后一个Dense层用于输出DNN部分的结果\n",
    "    deep_part = Dense(1)(deep_part)\n",
    "\n",
    "    # 将线性部分、FM部分和DNN部分的输出相加\n",
    "    # 然后通过Sigmoid激活函数将输出转换为概率\n",
    "    output = tf.keras.activations.sigmoid(linear_part + fm_part + deep_part)\n",
    "\n",
    "    # 构建整个DeepFM模型\n",
    "    model = Model(inputs, output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdfa91db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All `axis` values to be kept must have known shape. Got axis: (-1,), input shape: [None], with unknown axis at index: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 34\u001b[0m\n\u001b[0;32m     26\u001b[0m preprocessing_layers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric1\u001b[39m\u001b[38;5;124m'\u001b[39m: layers\u001b[38;5;241m.\u001b[39mNormalization(),\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric2\u001b[39m\u001b[38;5;124m'\u001b[39m: layers\u001b[38;5;241m.\u001b[39mNormalization(),\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory1\u001b[39m\u001b[38;5;124m'\u001b[39m: layers\u001b[38;5;241m.\u001b[39mStringLookup(output_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone_hot\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory2\u001b[39m\u001b[38;5;124m'\u001b[39m: layers\u001b[38;5;241m.\u001b[39mStringLookup(output_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone_hot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m }\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 应用预处理\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [preprocessing_layers[name](inputs[name]) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m     35\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConcatenate()(preprocessed)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Wide 部分\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 34\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     26\u001b[0m preprocessing_layers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric1\u001b[39m\u001b[38;5;124m'\u001b[39m: layers\u001b[38;5;241m.\u001b[39mNormalization(),\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric2\u001b[39m\u001b[38;5;124m'\u001b[39m: layers\u001b[38;5;241m.\u001b[39mNormalization(),\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory1\u001b[39m\u001b[38;5;124m'\u001b[39m: layers\u001b[38;5;241m.\u001b[39mStringLookup(output_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone_hot\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory2\u001b[39m\u001b[38;5;124m'\u001b[39m: layers\u001b[38;5;241m.\u001b[39mStringLookup(output_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone_hot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m }\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 应用预处理\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [preprocessing_layers[name](inputs[name]) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m     35\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConcatenate()(preprocessed)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Wide 部分\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\layers\\preprocessing\\normalization.py:187\u001b[0m, in \u001b[0;36mNormalization.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keep_axis:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_shape[d] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll `axis` values to be kept must have known shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot axis: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput shape: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, with unknown axis at index: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    191\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis, input_shape, d\n\u001b[0;32m    192\u001b[0m             )\n\u001b[0;32m    193\u001b[0m         )\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Axes to be reduced.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce_axis \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ndim) \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keep_axis]\n",
      "\u001b[1;31mValueError\u001b[0m: All `axis` values to be kept must have known shape. Got axis: (-1,), input shape: [None], with unknown axis at index: 0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 生成模拟数据\n",
    "num_samples = 10000\n",
    "data = {\n",
    "    'category1': np.random.choice(['A', 'B', 'C'], num_samples),\n",
    "    'category2': np.random.choice(['X', 'Y', 'Z'], num_samples),\n",
    "    'numeric1': np.random.randn(num_samples),\n",
    "    'numeric2': np.random.randn(num_samples),\n",
    "    'target': np.random.randint(0, 2, num_samples)\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# 分割数据集\n",
    "train, val = np.split(data.sample(frac=1), [int(0.8*len(data))])\n",
    "\n",
    "# 创建输入层\n",
    "inputs = {col: tf.keras.Input(name=col, shape=(), dtype='float32') for col in ['numeric1', 'numeric2']}\n",
    "inputs.update({col: tf.keras.Input(name=col, shape=(), dtype='string') for col in ['category1', 'category2']})\n",
    "\n",
    "# 预处理层\n",
    "preprocessing_layers = {\n",
    "    'numeric1': layers.Normalization(),\n",
    "    'numeric2': layers.Normalization(),\n",
    "    'category1': layers.StringLookup(output_mode='one_hot'),\n",
    "    'category2': layers.StringLookup(output_mode='one_hot')\n",
    "}\n",
    "\n",
    "# 应用预处理\n",
    "preprocessed = [preprocessing_layers[name](inputs[name]) for name in inputs]\n",
    "preprocessed = layers.Concatenate()(preprocessed)\n",
    "\n",
    "# Wide 部分\n",
    "wide_output = layers.Dense(1, activation='sigmoid')(preprocessed)\n",
    "\n",
    "# Deep 部分\n",
    "deep = layers.Dense(128, activation='relu')(preprocessed)\n",
    "deep = layers.Dense(128, activation='relu')(deep)\n",
    "deep = layers.Dropout(0.3)(deep)\n",
    "deep_output = layers.Dense(1, activation='sigmoid')(deep)\n",
    "\n",
    "# 结合 Wide 和 Deep 部分\n",
    "combined = layers.concatenate([deep_output, wide_output])\n",
    "\n",
    "# 输出层\n",
    "output_layer = layers.Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "# 创建模型\n",
    "model = tf.keras.Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 转换数据\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    df = dataframe.copy()\n",
    "    labels = df.pop('target')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# 训练模型\n",
    "model.fit(train_ds, epochs=10, validation_data=val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fea5ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " feature4 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " feature5 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " feature6 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1, 4)         40          ['feature4[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 4)         80          ['feature5[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 4)         60          ['feature6[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1, 12)        0           ['embedding_1[0][0]',            \n",
      "                                                                  'embedding_2[0][0]',            \n",
      "                                                                  'embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 12)           0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           832         ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " wide_input (InputLayer)        [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 32)           2080        ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 35)           0           ['wide_input[0][0]',             \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            36          ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,128\n",
      "Trainable params: 3,128\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 1s 11ms/step - loss: 0.7248 - accuracy: 0.4925 - val_loss: 0.7287 - val_accuracy: 0.4850\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7204 - accuracy: 0.4950 - val_loss: 0.7265 - val_accuracy: 0.4750\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7157 - accuracy: 0.5013 - val_loss: 0.7238 - val_accuracy: 0.4800\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7102 - accuracy: 0.5038 - val_loss: 0.7207 - val_accuracy: 0.4850\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7031 - accuracy: 0.5213 - val_loss: 0.7194 - val_accuracy: 0.4600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2cc9ee998d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 假设你有一个包含wide和deep特征的数据集\n",
    "# 请替换下面的伪造数据为你自己的数据\n",
    "wide_features = tf.random.normal((1000, 3))\n",
    "deep_features = {\n",
    "    'feature4': tf.random.uniform((1000, 1), maxval=10, dtype=tf.int32),\n",
    "    'feature5': tf.random.uniform((1000, 1), maxval=20, dtype=tf.int32),\n",
    "    'feature6': tf.random.uniform((1000, 1), maxval=15, dtype=tf.int32)\n",
    "}\n",
    "\n",
    "labels = tf.random.uniform((1000, 1), maxval=2, dtype=tf.int32)\n",
    "\n",
    "# 定义 wide 特征的输入\n",
    "wide_inputs = Input(shape=(3,), name='wide_input')\n",
    "\n",
    "# 定义 deep 特征的输入\n",
    "deep_inputs = {feature: Input(shape=(1,), name=feature) for feature in ['feature4', 'feature5', 'feature6']}\n",
    "\n",
    "# 定义 wide 部分\n",
    "wide_branch = wide_inputs\n",
    "\n",
    "# 定义 deep 部分\n",
    "embeddings = [Embedding(input_dim=deep_features[feature].numpy().max() + 1, output_dim=4)(deep_inputs[feature]) for feature in ['feature4', 'feature5', 'feature6']]\n",
    "deep_branch = concatenate(embeddings)\n",
    "deep_branch = tf.keras.layers.Flatten()(deep_branch)\n",
    "deep_branch = Dense(64, activation='relu')(deep_branch)\n",
    "deep_branch = Dense(32, activation='relu')(deep_branch)\n",
    "\n",
    "# 合并 wide 和 deep 部分\n",
    "combined = concatenate([wide_branch, deep_branch])\n",
    "\n",
    "# 输出层\n",
    "output = Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "# 编译模型\n",
    "model = Model(inputs=[wide_inputs] + list(deep_inputs.values()), outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 打印模型结构\n",
    "model.summary()\n",
    "\n",
    "# 训练模型\n",
    "model.fit([wide_features] + list(deep_features.values()), labels, epochs=5, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0512d",
   "metadata": {},
   "source": [
    "Wide & Deep 模型是一种结合了线性模型和深度神经网络的机器学习模型。这种模型由 Google Play 团队提出，用于改进推荐系统的效果。其核心思想是同时使用宽（Wide）和深（Deep）两个部分，以达到更好的预测效果。\n",
    "\n",
    "1. **Wide 部分**：这部分是一个线性模型。它的主要作用是记忆性（memorization），即通过特征的线性组合来捕捉实体之间的直接关系。这些特征直接连接到模型的输出层。在推荐系统中，这意味着模型可以有效地利用用户和物品之间的直接关联（例如，用户过去购买或点击过的物品）。\n",
    "\n",
    "2. **Deep 部分**：这部分是一个深度神经网络，通常包括多个隐藏层。它的主要作用是泛化（generalization），即通过特征的非线性转换来发现实体间更加抽象的关系。在这个部分，特征通常会首先通过嵌入（embedding）层转换成密集的特征向量，然后通过多层感知机（MLP）进行进一步的处理。\n",
    "\n",
    "Wide & Deep 模型的优势在于它结合了两种方法的优点：线性模型的高效记忆能力和深度学习的强大泛化能力。这使得它在处理复杂的推荐系统任务时，既能捕捉到项目间的直接相关性，又能通过深度学习发现潜在的、非显而易见的模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dea5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
