{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b601454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python function elapsed time: 0.0015057999989949167\n",
      "TensorFlow function elapsed time: 0.12585899999248795\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "# 定义一个简单的Python函数\n",
    "def simple_function(x):\n",
    "    return x * x + 2 * x + 1\n",
    "\n",
    "# 使用tf.function装饰器将其转换为TensorFlow函数\n",
    "@tf.function\n",
    "def tf_simple_function(x):\n",
    "    return x * x + 2 * x + 1\n",
    "\n",
    "# 测试执行性能\n",
    "input_data = tf.constant([1, 2, 3, 4, 5], dtype=tf.float32)\n",
    "\n",
    "# 测试普通Python函数的执行时间\n",
    "start_time = timeit.default_timer()\n",
    "_ = simple_function(input_data)\n",
    "elapsed_time_python = timeit.default_timer() - start_time\n",
    "\n",
    "# 测试TensorFlow函数的执行时间\n",
    "start_time = timeit.default_timer()\n",
    "_ = tf_simple_function(input_data)\n",
    "elapsed_time_tf = timeit.default_timer() - start_time\n",
    "\n",
    "print(\"Python function elapsed time:\", elapsed_time_python)\n",
    "print(\"TensorFlow function elapsed time:\", elapsed_time_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "050a74c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "普通函数执行时间: 0.0002205371856689453\n",
      "经过 @tf.function 装饰的函数执行时间: 0.3167226314544678\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# 普通函数\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "# 使用 @tf.function 装饰的函数\n",
    "@tf.function\n",
    "def tf_add(a, b):\n",
    "    return a + b\n",
    "\n",
    "# 测试普通函数性能\n",
    "start_time = time.time()\n",
    "for _ in range(1000):\n",
    "    result_add = add(1, 2)\n",
    "end_time = time.time()\n",
    "print(\"普通函数执行时间:\", end_time - start_time)\n",
    "\n",
    "# 测试经过 @tf.function 装饰的函数性能\n",
    "start_time = time.time()\n",
    "for _ in range(1000):\n",
    "    result_tf_add = tf_add(1, 2)\n",
    "end_time = time.time()\n",
    "print(\"经过 @tf.function 装饰的函数执行时间:\", end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7cf4901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "普通函数执行时间: 8.272811889648438\n",
      "经过 @tf.function 装饰的函数执行时间: 8.67142629623413\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# 普通函数\n",
    "def matmul(a, b):\n",
    "    return tf.matmul(a, b)\n",
    "\n",
    "# 使用 @tf.function 装饰的函数\n",
    "@tf.function\n",
    "def tf_matmul(a, b):\n",
    "    return tf.matmul(a, b)\n",
    "\n",
    "# 生成一些随机矩阵\n",
    "a = tf.random.normal((1000, 1000))\n",
    "b = tf.random.normal((1000, 1000))\n",
    "\n",
    "# 测试普通函数性能\n",
    "start_time = time.time()\n",
    "for _ in range(1000):\n",
    "    result_matmul = matmul(a, b)\n",
    "end_time = time.time()\n",
    "print(\"普通函数执行时间:\", end_time - start_time)\n",
    "\n",
    "# 测试经过 @tf.function 装饰的函数性能\n",
    "start_time = time.time()\n",
    "for _ in range(1000):\n",
    "    result_tf_matmul = tf_matmul(a, b)\n",
    "end_time = time.time()\n",
    "print(\"经过 @tf.function 装饰的函数执行时间:\", end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde78a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 构造一个简单的图像数据\n",
    "image = np.random.rand(1, 64, 64, 3)  # 一张64x64的RGB图像\n",
    "\n",
    "# 构建只包含一个Conv2D层的模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3))\n",
    "])\n",
    "\n",
    "# 获取Conv2D层的输出\n",
    "conv_output = model.predict(image)\n",
    "\n",
    "# 打印输入和输出\n",
    "print(\"Input (Image):\", image.shape)\n",
    "print(\"Output (Conv2D):\", conv_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c54b0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (Image): (1, 64, 64, 3)\n",
      "Output (Conv2D): (1, 62, 62, 16)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb29d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n",
      "Input (Image): (1, 64, 64, 3)\n",
      "Output (Conv2D + MaxPooling2D): (1, 31, 31, 16)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 构造一个简单的图像数据\n",
    "image = np.random.rand(1, 64, 64, 3)  # 一张64x64的RGB图像\n",
    "\n",
    "# 构建包含Conv2D和MaxPooling2D层的模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "])\n",
    "\n",
    "# 获取池化层的输出\n",
    "pooling_output = model.predict(image)\n",
    "\n",
    "# 打印输入和输出\n",
    "print(\"Input (Image):\", image.shape)\n",
    "print(\"Output (Conv2D + MaxPooling2D):\", pooling_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7557f0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data: [[1. 2. 3.]]\n",
      "Input Data shape: (1, 3)\n",
      "Output Data (After Dense Layer): [[3.9061089]]\n",
      "Input Data (After Dense Layer) shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 构造一些示例输入数据\n",
    "input_data = tf.constant([[1.0, 2.0, 3.0]])\n",
    "\n",
    "# 定义一个Dense层\n",
    "dense_layer = tf.keras.layers.Dense(units=1, activation='relu')\n",
    "\n",
    "# 将输入数据传递给Dense层\n",
    "output_data = dense_layer(input_data)\n",
    "\n",
    "# 打印输出\n",
    "print(\"Input Data:\", input_data.numpy())\n",
    "print(\"Input Data shape:\", input_data.shape)\n",
    "\n",
    "print(\"Output Data (After Dense Layer):\", output_data.numpy())\n",
    "print(\"Input Data (After Dense Layer) shape:\", output_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9446c9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data: [[1. 2. 3.]]\n",
      "Output Data (After Dense + BatchNormalization): [[0.9995004 1.9990008 2.9985013]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 构造一些示例输入数据\n",
    "input_data = tf.constant([[1.0, 2.0, 3.0]])\n",
    "\n",
    "# 定义一个Dense层，并在其后添加BatchNormalization层\n",
    "batch_norm_layer = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "# 将输入数据传递给Dense层，然后通过BatchNormalization层\n",
    "output_data = batch_norm_layer(input_data)\n",
    "\n",
    "# 打印输出\n",
    "print(\"Input Data:\", input_data.numpy())\n",
    "print(\"Output Data (After Dense + BatchNormalization):\", output_data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af1db1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data: [[1. 2. 3.]]\n",
      "Output Data (After Dropout): [[0. 0. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 构造一些示例输入数据\n",
    "input_data = tf.constant([[1.0, 2.0, 3.0]])\n",
    "\n",
    "# 定义一个Dense层，并在其后添加Dropout层\n",
    "dropout_layer = tf.keras.layers.Dropout(rate=0.5)\n",
    "\n",
    "# 将输入数据传递给Dense层，然后通过Dropout层\n",
    "output_data = dropout_layer(input_data, training=True)\n",
    "\n",
    "# 打印输出\n",
    "print(\"Input Data:\", input_data.numpy())\n",
    "print(\"Output Data (After Dropout):\", output_data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "680ffa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data: [[1. 2. 3.]]\n",
      "Output Data (After Dense + Dropout): [[4.078701  2.7408566 0.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 构造一些示例输入数据\n",
    "input_data = tf.constant([[1.0, 2.0, 3.0]])\n",
    "\n",
    "# 定义一个Dense层，并在其后添加Dropout层\n",
    "dense_layer = tf.keras.layers.Dense(units=4, activation='relu')\n",
    "dropout_layer = tf.keras.layers.Dropout(rate=0.5)\n",
    "\n",
    "# 将输入数据传递给Dense层，然后通过Dropout层\n",
    "output_data = dropout_layer(dense_layer(input_data), training=True)\n",
    "\n",
    "# 打印输出\n",
    "print(\"Input Data:\", input_data.numpy())\n",
    "print(\"Output Data (After Dense + Dropout):\", output_data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ecab4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data: [[1 2 3 4]]\n",
      "Input Data shape: (1, 4)\n",
      "Output Data (After Embedding): [[[ 0.0390268   0.03838977  0.03456748]\n",
      "  [-0.02889923  0.03671585 -0.0343789 ]\n",
      "  [-0.0006891   0.01629157 -0.02375256]\n",
      "  [-0.04163213 -0.02001661  0.04961659]]]\n",
      "Output Data shape: (1, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 构造一些示例输入数据\n",
    "input_data = tf.constant([[1, 2, 3, 4]])\n",
    "\n",
    "# 定义一个Embedding层\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=5, output_dim=3, input_length=4)\n",
    "\n",
    "# 将输入数据传递给Embedding层\n",
    "output_data = embedding_layer(input_data)\n",
    "\n",
    "# 打印输出\n",
    "print(\"Input Data:\", input_data.numpy())\n",
    "print(\"Input Data shape:\", input_data.shape)\n",
    "\n",
    "print(\"Output Data (After Embedding):\", output_data.numpy())\n",
    "print(\"Output Data shape:\", output_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ff3d47a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"global_average_pooling2d\" is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (1, 3, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m global_avg_pooling_layer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mGlobalAveragePooling2D()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 将输入数据传递给全局平均池化层\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m output_data \u001b[38;5;241m=\u001b[39m global_avg_pooling_layer(input_data)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 打印输入和输出\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_data\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\input_spec.py:235\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    233\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    237\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m         )\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"global_average_pooling2d\" is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (1, 3, 3)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 构造一些示例输入数据\n",
    "input_data = tf.constant([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "\n",
    "# 定义一个全局平均池化层\n",
    "global_avg_pooling_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "# 将输入数据传递给全局平均池化层\n",
    "output_data = global_avg_pooling_layer(input_data)\n",
    "\n",
    "# 打印输入和输出\n",
    "print(\"Input Data:\", input_data.numpy())\n",
    "print(\"Output Data (After Global Average Pooling):\", output_data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8432da75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data: [[[1. 2. 3.]\n",
      "  [4. 5. 6.]]]\n",
      "Input Data: (1, 2, 3)\n",
      "Input Data: [[[[1. 2. 3.]\n",
      "   [4. 5. 6.]]]]\n",
      "Input Data: (1, 1, 2, 3)\n",
      "Output Data (After Global Average Pooling): [[4. 5. 6.]]\n",
      "Output Data: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 构造一些示例输入数据\n",
    "input_data = tf.constant([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]])\n",
    "print(\"Input Data:\", input_data.numpy())\n",
    "print(\"Input Data:\", input_data.shape)\n",
    "\n",
    "# 添加一个维度来匹配GlobalAveragePooling2D的期望输入形状\n",
    "input_data = tf.expand_dims(input_data, axis=0)\n",
    "\n",
    "# 定义一个全局平均池化层\n",
    "global_avg_pooling_layer = tf.keras.layers.GlobalMaxPooling2D()\n",
    "\n",
    "# 将输入数据传递给全局平均池化层\n",
    "output_data = global_avg_pooling_layer(input_data)\n",
    "\n",
    "# 打印输入和输出\n",
    "print(\"Input Data:\", input_data.numpy())\n",
    "print(\"Input Data:\", input_data.shape)\n",
    "\n",
    "print(\"Output Data (After Global Average Pooling):\", output_data.numpy())\n",
    "print(\"Output Data:\", output_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb8346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0a192f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"sequential_9\" is incompatible with the layer: expected shape=(None, 3, 3, 1), found shape=(1, 2, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 13\u001b[0m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      8\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m16\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)),\n\u001b[0;32m      9\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mGlobalAveragePooling2D()\n\u001b[0;32m     10\u001b[0m ])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 将输入数据传递给模型\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m output_data \u001b[38;5;241m=\u001b[39m model(input_data)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 打印输入和输出\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_data\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\input_spec.py:298\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 298\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    299\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"sequential_9\" is incompatible with the layer: expected shape=(None, 3, 3, 1), found shape=(1, 2, 3)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 构造一些示例输入数据\n",
    "input_data = tf.constant([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]])\n",
    "\n",
    "# 定义一个卷积神经网络模型，包括卷积层和全局平均池化层\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(3, 3, 1)),\n",
    "    tf.keras.layers.GlobalAveragePooling2D()\n",
    "])\n",
    "\n",
    "# 将输入数据传递给模型\n",
    "output_data = model(input_data)\n",
    "\n",
    "# 打印输入和输出\n",
    "print(\"Input Data:\", input_data.numpy())\n",
    "print(\"Input Data Shape:\", input_data.shape)\n",
    "print(\"Output Data (After Global Average Pooling):\", output_data.numpy())\n",
    "print(\"Output Data Shape:\", output_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0d980e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 222, 222, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 111, 111, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 109, 109, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 54, 54, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 52, 52, 256)       295168    \n",
      "                                                                 \n",
      " global_average_pooling2d_6   (None, 256)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 437,379\n",
      "Trainable params: 437,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 构建一个简单的CNN模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=3, activation='softmax')\n",
    "])\n",
    "\n",
    "# 打印模型的结构\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0cd346cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data: [[[[1. 2. 3.]\n",
      "   [4. 5. 6.]\n",
      "   [7. 8. 9.]]]]\n",
      "Output Data (After Global Max Pooling): [[7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 构造一些示例输入数据\n",
    "input_data = tf.constant([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "\n",
    "# 添加一个维度来匹配GlobalMaxPooling2D的期望输入形状\n",
    "input_data = tf.expand_dims(input_data, axis=0)\n",
    "\n",
    "# 定义一个全局最大池化层\n",
    "global_max_pooling_layer = tf.keras.layers.GlobalMaxPooling2D()\n",
    "\n",
    "# 将输入数据传递给全局最大池化层\n",
    "output_data = global_max_pooling_layer(input_data)\n",
    "\n",
    "# 打印输入和输出\n",
    "print(\"Input Data:\", input_data.numpy())\n",
    "print(\"Output Data (After Global Max Pooling):\", output_data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9faa81a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data 1: [[1. 2. 3.]]\n",
      "Input Data 2: [[4. 5. 6.]]\n",
      "Output Data (After Concatenation): [[1. 2. 3. 4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 构造两个示例输入数据\n",
    "input_data1 = tf.constant([[1.0, 2.0, 3.0]])\n",
    "input_data2 = tf.constant([[4.0, 5.0, 6.0]])\n",
    "\n",
    "# 定义一个连接层\n",
    "concatenate_layer = tf.keras.layers.Concatenate(axis=-1)\n",
    "\n",
    "# 将两个输入数据传递给连接层\n",
    "output_data = concatenate_layer([input_data1, input_data2])\n",
    "\n",
    "# 打印输入和输出\n",
    "print(\"Input Data 1:\", input_data1.numpy())\n",
    "print(\"Input Data 2:\", input_data2.numpy())\n",
    "print(\"Output Data (After Concatenation):\", output_data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6bb6771e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data 1: [[1. 2. 3.]]\n",
      "Input Data 2: [[4. 5. 6.]]\n",
      "Output Data (After Concatenation): [[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 构造两个示例输入数据\n",
    "input_data1 = tf.constant([[1.0, 2.0, 3.0]])\n",
    "input_data2 = tf.constant([[4.0, 5.0, 6.0]])\n",
    "\n",
    "# 定义一个连接层，将在第二个维度上进行连接\n",
    "concatenate_layer = tf.keras.layers.Concatenate(axis=0)\n",
    "\n",
    "# 将两个输入数据传递给连接层\n",
    "output_data = concatenate_layer([input_data1, input_data2])\n",
    "\n",
    "# 打印输入和输出\n",
    "print(\"Input Data 1:\", input_data1.numpy())\n",
    "print(\"Input Data 2:\", input_data2.numpy())\n",
    "print(\"Output Data (After Concatenation):\", output_data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0f4ce40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11b793a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8181818181818181, 0.75)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义表格中的数据\n",
    "data = {\n",
    "    \"female\": {\"treatment_A_remission\": 50, \"treatment_A_no_remission\": 50, \"treatment_B_remission\": 220, \"treatment_B_no_remission\": 180},\n",
    "    \"male\": {\"treatment_A_remission\": 300, \"treatment_A_no_remission\": 100, \"treatment_B_remission\": 80, \"treatment_B_no_remission\": 20}\n",
    "}\n",
    "\n",
    "# 计算女性的比值比（Odds Ratio）\n",
    "OR_female = (data[\"female\"][\"treatment_A_remission\"] / data[\"female\"][\"treatment_A_no_remission\"]) / (data[\"female\"][\"treatment_B_remission\"] / data[\"female\"][\"treatment_B_no_remission\"])\n",
    "\n",
    "# 计算男性的比值比（Odds Ratio）\n",
    "OR_male = (data[\"male\"][\"treatment_A_remission\"] / data[\"male\"][\"treatment_A_no_remission\"]) / (data[\"male\"][\"treatment_B_remission\"] / data[\"male\"][\"treatment_B_no_remission\"])\n",
    "\n",
    "OR_female, OR_male\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f387d0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5555555555555556,\n",
       " array([[350, 150],\n",
       "        [300, 200]]),\n",
       " 0.001159465691351218,\n",
       " array([0.57718174, 0.97456306]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "# 首先，我们需要构建边际表\n",
    "# 从原始数据中提取总体的缓解和未缓解的人数，不分性别\n",
    "total_remission_A = data[\"female\"][\"treatment_A_remission\"] + data[\"male\"][\"treatment_A_remission\"]\n",
    "total_no_remission_A = data[\"female\"][\"treatment_A_no_remission\"] + data[\"male\"][\"treatment_A_no_remission\"]\n",
    "total_remission_B = data[\"female\"][\"treatment_B_remission\"] + data[\"male\"][\"treatment_B_remission\"]\n",
    "total_no_remission_B = data[\"female\"][\"treatment_B_no_remission\"] + data[\"male\"][\"treatment_B_no_remission\"]\n",
    "\n",
    "# 边际表\n",
    "marginal_table = np.array([[total_remission_A, total_no_remission_A], \n",
    "                            [total_remission_B, total_no_remission_B]])\n",
    "\n",
    "# 计算边际比值比\n",
    "marginal_OR = (marginal_table[0][0] / marginal_table[0][1]) / (marginal_table[1][0] / marginal_table[1][1])\n",
    "\n",
    "# 计算χ²检验，以判断辛普森悖论\n",
    "chi2, p, dof, ex = chi2_contingency(marginal_table)\n",
    "\n",
    "# 使用χ²检验的结果计算男性部分比值比的95%置信区间\n",
    "# 计算标准误差 (se) 和 z值 (z_value)\n",
    "se = np.sqrt(1/total_remission_A + 1/total_no_remission_A + 1/total_remission_B + 1/total_no_remission_B)\n",
    "z_value = 1.96  # 对应95%置信区间的z值\n",
    "\n",
    "# 男性的部分比值比 (OR_male) 已经在上一个计算中得出\n",
    "# 计算置信区间的对数变换\n",
    "log_OR_male = np.log(OR_male)\n",
    "conf_interval_log = [log_OR_male - z_value * se, log_OR_male + z_value * se]\n",
    "\n",
    "# 将对数置信区间转换回原始比值比\n",
    "conf_interval_OR_male = np.exp(conf_interval_log)\n",
    "\n",
    "marginal_OR, marginal_table, p, conf_interval_OR_male\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad81760c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3877"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "346 + 386 + 338 + 380 + 418 + 401 + 373 + 371 + 437 + 427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61fae67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Total: 387.7\n",
      "SSB: 9836.1\n",
      "SSW: 9836.1\n",
      "df_B: 9\n",
      "df_W: 9\n",
      "MSB: 1092.9\n",
      "MSW: 1092.9\n",
      "F-statistic: 1.0\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# 数据\n",
    "thefts_by_year = [346, 386, 338, 380, 418, 401, 373, 371, 437, 427]\n",
    "\n",
    "# 计算全体均值\n",
    "mean_total = sum(thefts_by_year) / len(thefts_by_year)\n",
    "\n",
    "# 计算组间平方和 (SSB)\n",
    "ssb = sum((1 * (x - mean_total) ** 2) for x in thefts_by_year)\n",
    "\n",
    "# 计算组内平方和 (SSW)\n",
    "ssw = sum((x - ) ** 2 for x in thefts_by_year)\n",
    "\n",
    "# 计算自由度\n",
    "df_b = len(thefts_by_year) - 1\n",
    "df_w = len(thefts_by_year) - 1\n",
    "\n",
    "# 计算均方\n",
    "msb = ssb / df_b\n",
    "msw = ssw / df_w\n",
    "\n",
    "# 计算F值\n",
    "f_statistic = msb / msw\n",
    "\n",
    "# 输出结果\n",
    "print(\"Mean Total:\", mean_total)\n",
    "print(\"SSB:\", ssb)\n",
    "print(\"SSW:\", ssw)\n",
    "print(\"df_B:\", df_b)\n",
    "print(\"df_W:\", df_w)\n",
    "print(\"MSB:\", msb)\n",
    "print(\"MSW:\", msw)\n",
    "print(\"F-statistic:\", f_statistic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87ee8d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "057f5de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20d66752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0cc441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " shared_embedding (Embedding)   (None, 10, 8)        800         ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 80)           0           ['shared_embedding[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 80)           0           ['shared_embedding[1][0]']       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           5184        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           5184        ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,168\n",
      "Trainable params: 11,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 13ms/step - loss: 0.7745 - accuracy: 0.4711 - val_loss: 0.7359 - val_accuracy: 0.4400\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.6416 - accuracy: 0.6533 - val_loss: 0.7102 - val_accuracy: 0.4800\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5969 - accuracy: 0.7733 - val_loss: 0.7059 - val_accuracy: 0.4900\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5624 - accuracy: 0.8278 - val_loss: 0.7152 - val_accuracy: 0.4800\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5329 - accuracy: 0.8644 - val_loss: 0.7213 - val_accuracy: 0.4500\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5056 - accuracy: 0.9000 - val_loss: 0.7313 - val_accuracy: 0.4700\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4810 - accuracy: 0.9244 - val_loss: 0.7310 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4572 - accuracy: 0.9300 - val_loss: 0.7455 - val_accuracy: 0.4900\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4347 - accuracy: 0.9433 - val_loss: 0.7549 - val_accuracy: 0.4400\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4131 - accuracy: 0.9467 - val_loss: 0.7593 - val_accuracy: 0.4600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.7745499610900879,\n",
       "  0.6415559649467468,\n",
       "  0.596912145614624,\n",
       "  0.5623939037322998,\n",
       "  0.5328953862190247,\n",
       "  0.5056374073028564,\n",
       "  0.4810194671154022,\n",
       "  0.457234263420105,\n",
       "  0.4347202181816101,\n",
       "  0.4130679666996002],\n",
       " 'accuracy': [0.47111111879348755,\n",
       "  0.653333306312561,\n",
       "  0.7733333110809326,\n",
       "  0.8277778029441833,\n",
       "  0.8644444346427917,\n",
       "  0.8999999761581421,\n",
       "  0.9244444370269775,\n",
       "  0.9300000071525574,\n",
       "  0.9433333277702332,\n",
       "  0.9466666579246521],\n",
       " 'val_loss': [0.7359145879745483,\n",
       "  0.7102096676826477,\n",
       "  0.7058979868888855,\n",
       "  0.7152442336082458,\n",
       "  0.7212607860565186,\n",
       "  0.7312759160995483,\n",
       "  0.7309934496879578,\n",
       "  0.7455120086669922,\n",
       "  0.7548627257347107,\n",
       "  0.7592551708221436],\n",
       " 'val_accuracy': [0.4399999976158142,\n",
       "  0.47999998927116394,\n",
       "  0.49000000953674316,\n",
       "  0.47999998927116394,\n",
       "  0.44999998807907104,\n",
       "  0.4699999988079071,\n",
       "  0.5,\n",
       "  0.49000000953674316,\n",
       "  0.4399999976158142,\n",
       "  0.46000000834465027]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dot\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# 定义超参数\n",
    "vocab_size = 100  # 词汇表大小\n",
    "embedding_dim = 8  # 嵌入维度\n",
    "max_length = 10  # 输入序列的最大长度\n",
    "num_samples = 1000  # 样本数量\n",
    "\n",
    "# 生成模拟数据\n",
    "np.random.seed(42)\n",
    "# 生成查询和文档的随机序列\n",
    "query_sequences = np.random.randint(1, vocab_size, size=(num_samples, max_length))\n",
    "doc_sequences = np.random.randint(1, vocab_size, size=(num_samples, max_length))\n",
    "# 生成标签，随机选择一部分为正样本（相似），其余为负样本（不相似）\n",
    "labels = np.random.randint(2, size=(num_samples, 1))\n",
    "\n",
    "# 构建模型\n",
    "query_input = Input(shape=(max_length,), dtype='int32')\n",
    "doc_input = Input(shape=(max_length,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name='shared_embedding')\n",
    "\n",
    "query_embedding = embedding_layer(query_input)\n",
    "doc_embedding = embedding_layer(doc_input)\n",
    "\n",
    "query_flat = Flatten()(query_embedding)\n",
    "doc_flat = Flatten()(doc_embedding)\n",
    "\n",
    "query_dense = Dense(64, activation='relu')(query_flat)\n",
    "doc_dense = Dense(64, activation='relu')(doc_flat)\n",
    "\n",
    "dot_similarity = Dot(axes=1, normalize=True)([query_dense, doc_dense])\n",
    "\n",
    "model = Model(inputs=[query_input, doc_input], outputs=dot_similarity)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 模型概况\n",
    "model.summary()\n",
    "\n",
    "# 拟合模型\n",
    "history = model.fit([query_sequences, doc_sequences], labels, batch_size=32, epochs=10, validation_split=0.1)\n",
    "\n",
    "history.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b236eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Dot\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aab8c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子以确保可复现性\n",
    "np.random.seed(42)\n",
    "\n",
    "# 定义参数\n",
    "num_users = 1000\n",
    "num_items = 1000\n",
    "embedding_size = 30\n",
    "\n",
    "# 生成用户和物品的ID\n",
    "user_ids = np.random.randint(0, num_users, size=(10000, 1))\n",
    "item_ids = np.random.randint(0, num_items, size=(10000, 1))\n",
    "\n",
    "# 随机生成标签（这里简化为0或1，表示用户对物品的喜好）\n",
    "labels = np.random.randint(2, size=(10000, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40369291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " user_input (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " item_input (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " user_embedding (Embedding)     (None, 1, 30)        30000       ['user_input[0][0]']             \n",
      "                                                                                                  \n",
      " item_embedding (Embedding)     (None, 1, 30)        30000       ['item_input[0][0]']             \n",
      "                                                                                                  \n",
      " flatten_user (Flatten)         (None, 30)           0           ['user_embedding[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_item (Flatten)         (None, 30)           0           ['item_embedding[0][0]']         \n",
      "                                                                                                  \n",
      " dot_product (Dot)              (None, 1)            0           ['flatten_user[0][0]',           \n",
      "                                                                  'flatten_item[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,000\n",
      "Trainable params: 60,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定义输入\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "# 定义嵌入层\n",
    "user_embedding = Embedding(num_users, embedding_size, input_length=1, name='user_embedding')(user_input)\n",
    "item_embedding = Embedding(num_items, embedding_size, input_length=1, name='item_embedding')(item_input)\n",
    "\n",
    "# 扁平化嵌入向量\n",
    "user_vec = Flatten(name='flatten_user')(user_embedding)\n",
    "item_vec = Flatten(name='flatten_item')(item_embedding)\n",
    "\n",
    "# 计算用户向量和物品向量的点积\n",
    "merged = Dot(name='dot_product', normalize=True, axes=1)([user_vec, item_vec])\n",
    "\n",
    "# 定义模型\n",
    "model = Model(inputs=[user_input, item_input], outputs=merged)\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 模型概览\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f03d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "141/141 [==============================] - 0s 3ms/step - loss: 2.8602 - accuracy: 0.7430 - val_loss: 4.4700 - val_accuracy: 0.5030\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 0s 3ms/step - loss: 2.7309 - accuracy: 0.7641 - val_loss: 4.5148 - val_accuracy: 0.5030\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 0s 2ms/step - loss: 2.6253 - accuracy: 0.7796 - val_loss: 4.4676 - val_accuracy: 0.5030\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 0s 2ms/step - loss: 2.5350 - accuracy: 0.7904 - val_loss: 4.5717 - val_accuracy: 0.5050\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 0s 2ms/step - loss: 2.4618 - accuracy: 0.8034 - val_loss: 4.6018 - val_accuracy: 0.5050\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 0s 2ms/step - loss: 2.3852 - accuracy: 0.8121 - val_loss: 4.5882 - val_accuracy: 0.5050\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 0s 3ms/step - loss: 2.3094 - accuracy: 0.8183 - val_loss: 4.5387 - val_accuracy: 0.5050\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 0s 2ms/step - loss: 2.2316 - accuracy: 0.8260 - val_loss: 4.4827 - val_accuracy: 0.5050\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 0s 2ms/step - loss: 2.1694 - accuracy: 0.8333 - val_loss: 4.4400 - val_accuracy: 0.5060\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 0s 3ms/step - loss: 2.1147 - accuracy: 0.8391 - val_loss: 4.4059 - val_accuracy: 0.5050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1de43c08390>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([user_ids, item_ids], labels, batch_size=64, epochs=10, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68da05f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step\n",
      "推荐给用户1的前10个物品ID: [934 425 677 240 773 362 149 447 679 665]\n"
     ]
    }
   ],
   "source": [
    "# 生成物品的ID序列\n",
    "item_ids_for_prediction = np.arange(num_items).reshape(-1, 1)\n",
    "\n",
    "# 预测用户对所有物品的兴趣\n",
    "predictions = model.predict([np.ones_like(item_ids_for_prediction) * 1, item_ids_for_prediction])\n",
    "\n",
    "# 获取预测兴趣最高的前10个物品\n",
    "top_10_items = predictions.flatten().argsort()[-10:][::-1]\n",
    "\n",
    "print(f\"推荐给用户1的前10个物品ID: {top_10_items}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f41dcd",
   "metadata": {},
   "source": [
    "是的，DSSM（Deep Structured Semantic Model，深度结构化语义模型）可以被视为一种“双塔”（Two-Tower）模型的早期例子。在双塔模型中，两个独立的神经网络（称为“塔”）分别处理两种不同类型的输入数据，通常是用户和物品的特征。这两个网络通常结构相似但是参数独立，它们的目标是将输入特征转换成嵌入向量（即低维稠密向量），这些嵌入向量随后被用来计算两个输入间的相似度或者相关性得分。\n",
    "\n",
    "### DSSM的双塔结构\n",
    "\n",
    "在DSSM中，一个“塔”处理查询（例如，用户的搜索查询），另一个“塔”处理文档（例如，网页或广告）。每个塔通常包括多层全连接网络，用于学习输入文本的深层语义表示。最终，两个塔的输出（即，查询和文档的嵌入向量）通过余弦相似度（或其他相似度度量）来计算它们之间的相似性。\n",
    "\n",
    "### 双塔模型的关键特点\n",
    "\n",
    "- **独立的特征学习**：两个塔独立学习输入数据的表示，这使得模型能够灵活处理不同类型的数据并学习它们的深层语义。\n",
    "- **相似度计算**：通过计算两个嵌入向量的相似度，双塔模型能够评估两个输入之间的关系，这对于推荐系统、信息检索和许多其他应用非常重要。\n",
    "- **可扩展性**：双塔模型特别适合大规模应用，因为一旦嵌入向量被计算出来，就可以高效地在大量候选中进行匹配和检索。\n",
    "\n",
    "### 应用\n",
    "\n",
    "DSSM及其变体在推荐系统、信息检索、自然语言处理等领域有广泛应用。双塔模型的一个关键优势是它们可以被预先训练和索引，从而在实际应用中实现快速的检索和匹配。\n",
    "\n",
    "总的来说，DSSM是双塔模型概念的早期实现之一，它展示了深度学习在理解文本语义相似度方面的强大能力。随着技术的发展，许多基于DSSM的改进和扩展模型已经被提出，进一步提高了模型的性能和应用范围。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c467d",
   "metadata": {},
   "source": [
    "DSSM（Deep Structured Semantic Model），即深度结构化语义模型，是一种基于深度学习的语义匹配模型，最早由微软研究院提出。它主要用于处理文本相似度计算问题，广泛应用于搜索引擎的相关性匹配、推荐系统中的内容推荐、自然语言处理领域的语义理解等任务。\n",
    "\n",
    "### 原理和架构\n",
    "\n",
    "DSSM模型的核心思想是将文本（如查询和文档）通过深度神经网络转换成低维稠密向量，也称为语义向量，然后通过计算这些向量之间的相似度（如余弦相似度）来评估文本之间的语义相似度。\n",
    "\n",
    "DSSM的基本架构通常包含以下几个步骤：\n",
    "\n",
    "1. **文本表示**：首先将原始文本转换为词向量，常用的方法包括词袋模型、One-hot编码等。\n",
    "\n",
    "2. **深度神经网络**：通过一个或多个隐藏层将词向量转换为低维的语义向量。这些隐藏层可以是全连接层，也可以是卷积神经网络(CNN)或循环神经网络(RNN)等结构。\n",
    "\n",
    "3. **相似度计算**：最后，使用余弦相似度或其他相似度度量方式来计算两个语义向量的相似度。\n",
    "\n",
    "### 训练过程\n",
    "\n",
    "在训练过程中，DSSM使用负采样的方式来提升模型的区分能力。具体来说，对于一个正样本（查询和相关文档的对），会随机选择若干个负样本（查询和不相关文档的对）。模型的目标是最大化正样本的相似度，同时最小化负样本的相似度。\n",
    "\n",
    "模型通常使用交叉熵损失函数来进行训练，优化的目标是提升正样本的预测概率，降低负样本的预测概率。\n",
    "\n",
    "### 应用和优点\n",
    "\n",
    "DSSM模型在处理文本相似度和语义匹配问题时具有明显的优势，能够有效捕捉文本的深层语义信息。它在搜索引擎的查询-文档匹配、推荐系统中的用户兴趣匹配、以及自然语言处理的多种任务中都有广泛的应用。\n",
    "\n",
    "### 缺点\n",
    "\n",
    "尽管DSSM在语义理解方面表现出色，但它也存在一些局限性，比如对高维稀疏输入的处理效率较低，以及模型训练和调参相对复杂等问题。此外，随着深度学习技术的发展，后续有许多模型在DSSM的基础上进行了改进和优化，如使用更复杂的网络结构（比如Transformer）来进一步提升模型的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f0eeb",
   "metadata": {},
   "source": [
    "处理 Apache Spark 中的数据倾斜问题是一个常见但复杂的挑战。数据倾斜指的是在分布式计算过程中，数据不均匀地分配在不同的节点上，导致部分节点工作量过大，而其他节点则相对空闲，进而影响整个作业的处理效率。以下是一些常用的方法来处理 Spark 中的数据倾斜：\n",
    "\n",
    "1. **增大并行度**：\n",
    "   - 通过调整`spark.sql.shuffle.partitions`（对于 DataFrame API）或`spark.default.parallelism`（对于 RDD API）的配置来增加任务的并行度，从而有更多的分区来分散数据，减轻倾斜。\n",
    "\n",
    "2. **过滤无用数据**：\n",
    "   - 在数据处理前尽可能过滤掉不需要的数据，减少数据量可以在一定程度上缓解数据倾斜。\n",
    "\n",
    "3. **采用随机前缀**：\n",
    "   - 对倾斜的键添加随机前缀，使其分散到不同的分区中。处理完成后再去除这些前缀。这种方法适用于倾斜键值过多的情况。\n",
    "\n",
    "4. **扩展倾斜键**：\n",
    "   - 对于倾斜严重的键，可以将其拆分成多个键，使其能够均匀分布到不同的分区中。\n",
    "\n",
    "5. **使用广播变量**：\n",
    "   - 如果数据倾斜是由于小表和大表的JOIN操作导致的，可以考虑将小表广播出去，这样就不需要对大表进行shuffle操作，从而避免了数据倾斜。\n",
    "\n",
    "6. **重分区（Repartition）或者增加分区数（Coalesce）**：\n",
    "   - 使用`repartition`方法可以根据键值重新分配数据，有助于数据更均匀地分布。`coalesce`方法则在减少分区数时避免全局shuffle，但可以在增加分区数时使用`repartition`来重新分配数据。\n",
    "\n",
    "7. **自定义分区器**：\n",
    "   - 如果默认的分区方式不能有效地处理数据倾斜，可以考虑实现自定义分区器，根据数据的实际分布来分配键值到不同的分区中。\n",
    "\n",
    "8. **对倾斜键进行采样**：\n",
    "   - 分析数据分布，对倾斜的键进行采样，然后根据采样结果调整数据处理逻辑，比如通过对倾斜键分割处理等方式来减轻倾斜。\n",
    "\n",
    "9. **限制笛卡尔积操作**：\n",
    "   - 避免使用会产生大量数据的操作，如笛卡尔积（Cartesian product），因为这些操作很容易产生数据倾斜。\n",
    "\n",
    "处理数据倾斜是一个需要根据具体情况来定制解决方案的过程。通常，识别倾斜的来源并对其进行针对性的优化是解决问题的关键。实践中，可能需要结合多种方法来有效地缓解数据倾斜问题。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
